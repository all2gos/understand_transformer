"Large Language Models(LLM) have taken ~the NLP community~ ~AI community~ the Whole World by storm"

And the elementary building block of large language models (the GPT ones) is the transformer. Therefore, in this project, I would like to focus on trying to understand how this architecture works and trying to create it from scratch.

I am very interested in understanding the workflow of this architecture, and therefore I would like to simultaneously have a solid understanding of the theoretical basis of its operation at the same time without forgetting that still the transformer remains a tool of my work and there is a certain limit to its knowledge, which is not necessary to cross.

There are several sources that I intend to use:

1. The publication that first described the transformer architecture: [Attention is All You Need](https://arxiv.org/abs/1706.03762) and the following important publications in the subject (most are listed in the repository from point 2)
2. A [repository on GitHub](https://github.com/Hannibal046/Awesome-LLM?tab=readme-ov-file) that is a treasure trove of knowledge on the subject of LLM
3. [Repository on GitHub](https://github.com/mlabonne/llm-course) being a course to LLM
4. A very [helpful website](https://bbycroft.net/llm?utm_source=tldrai) in the whole learning process showing the operation of the Transformer interactively
5. Countless articles (also videos) scattered in various corners of the Internet (mainly [this excellent tutorial](https://www.youtube.com/watch?v=kCc8FmEb1nY)]
6. [Endless conversations with ChatGPT]


I invite you to familiarize yourself with my attempt to understand this one of the most popular architectures in use today. 


